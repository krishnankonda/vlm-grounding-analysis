{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Comparative Analysis of Specialist vs. Generalist Vision-Language Models on the Flickr30k Entities Grounding Benchmark\n",
        "\n",
        "This notebook implements a research project comparing specialist (GLIP) and generalist (LLaVA) vision-language models on the Flickr30k Entities grounding benchmark.\n",
        "\n",
        "**Inspiration**: This project is directly inspired by the foundational paper \"Flickr30k Entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\" (Plummer et al., 2015).\n",
        "\n",
        "**Core Goal**: Test whether modern SOTA \"generalist\" models (like LLaVA) have incidentally learned fine-grained grounding skills compared to \"specialist\" models (like GLIP) that were explicitly trained for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Environment and Data Preparation\n",
        "\n",
        "### 1.1 Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Data Acquisition\n",
        "\n",
        "Download the Flickr30k Entities dataset. You can either:\n",
        "1. Download manually from https://github.com/BryanPlummer/flickr30k_entities\n",
        "2. Use the code below to download programmatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "import requests\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "annotations_urls = [\n",
        "    \"https://raw.githubusercontent.com/BryanPlummer/flickr30k_entities/master/annotations.zip\",\n",
        "    \"https://github.com/BryanPlummer/flickr30k_entities/raw/master/annotations.zip\"\n",
        "]\n",
        "images_url = \"http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k-images.tar.gz\"\n",
        "\n",
        "annotations_path = \"data/annotations.zip\"\n",
        "images_path = \"data/flickr30k-images.tar.gz\"\n",
        "\n",
        "if not os.path.exists(\"data/Flickr30kEntities\"):\n",
        "    print(\"Downloading annotations...\")\n",
        "    downloaded = False\n",
        "    for url in annotations_urls:\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(annotations_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(\"Annotations downloaded!\")\n",
        "            downloaded = True\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Failed with {url}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if not downloaded:\n",
        "        raise Exception(\"Failed to download annotations from all URLs\")\n",
        "    \n",
        "    print(\"Extracting annotations...\")\n",
        "    with zipfile.ZipFile(annotations_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/\")\n",
        "    print(\"Annotations extracted!\")\n",
        "else:\n",
        "    print(\"Annotations already exist.\")\n",
        "\n",
        "if not os.path.exists(\"data/flickr30k-images\"):\n",
        "    if os.path.exists(images_path):\n",
        "        print(\"Found existing images archive, extracting...\")\n",
        "        with tarfile.open(images_path, \"r:gz\") as tar:\n",
        "            tar.extractall(\"data/\")\n",
        "        print(\"Images extracted!\")\n",
        "    else:\n",
        "        print(\"Downloading images...\")\n",
        "        try:\n",
        "            response = requests.get(images_url, stream=True, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            with open(images_path, 'wb') as f:\n",
        "                if total_size > 0:\n",
        "                    downloaded = 0\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        downloaded += len(chunk)\n",
        "                        if downloaded % (10 * 1024 * 1024) == 0:\n",
        "                            print(f\"Downloaded {downloaded / (1024*1024):.1f} MB...\")\n",
        "                else:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(\"Images downloaded!\")\n",
        "            print(\"Extracting images...\")\n",
        "            with tarfile.open(images_path, \"r:gz\") as tar:\n",
        "                tar.extractall(\"data/\")\n",
        "            print(\"Images extracted!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download images automatically: {e}\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"MANUAL DOWNLOAD REQUIRED:\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"The Flickr30k images require manual download.\")\n",
        "            print(\"Please visit: http://shannon.cs.illinois.edu/DenotationGraph/\")\n",
        "            print(\"Fill out the form to request access, then download flickr30k-images.tar.gz\")\n",
        "            print(f\"Place it at: {images_path}\")\n",
        "            print(\"Then re-run this cell to extract the images.\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"\\nNote: The code will automatically extract the file if you place it at: {images_path}\")\n",
        "else:\n",
        "    print(\"Images already exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Annotation Parsing\n",
        "\n",
        "Parse the XML annotation files to extract image-phrase-bounding-box triplets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_flickr30k_xml(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    filename = root.find('filename').text\n",
        "    results = []\n",
        "    \n",
        "    for obj in root.findall('object'):\n",
        "        name_elem = obj.find('name')\n",
        "        bndbox_elem = obj.find('bndbox')\n",
        "        \n",
        "        if name_elem is not None and bndbox_elem is not None:\n",
        "            phrase = name_elem.text\n",
        "            xmin = int(bndbox_elem.find('xmin').text)\n",
        "            ymin = int(bndbox_elem.find('ymin').text)\n",
        "            xmax = int(bndbox_elem.find('xmax').text)\n",
        "            ymax = int(bndbox_elem.find('ymax').text)\n",
        "            \n",
        "            results.append({\n",
        "                \"image_filename\": filename,\n",
        "                \"phrase\": phrase,\n",
        "                \"bbox\": [xmin, ymin, xmax, ymax]\n",
        "            })\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "annotations_dir = \"data/Flickr30kEntities/Annotations\"\n",
        "all_groundings = []\n",
        "\n",
        "xml_files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
        "print(f\"Found {len(xml_files)} XML annotation files\")\n",
        "\n",
        "for xml_file in tqdm(xml_files, desc=\"Parsing XML files\"):\n",
        "    xml_path = os.path.join(annotations_dir, xml_file)\n",
        "    groundings = parse_flickr30k_xml(xml_path)\n",
        "    all_groundings.extend(groundings)\n",
        "\n",
        "print(f\"\\nTotal groundings extracted: {len(all_groundings)}\")\n",
        "print(f\"Sample grounding: {all_groundings[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Test Set Creation\n",
        "\n",
        "Create a reproducible 500-sample test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_all = pd.DataFrame(all_groundings)\n",
        "print(f\"Full dataset size: {len(df_all)} rows\")\n",
        "\n",
        "test_set = df_all.sample(n=500, random_state=42)\n",
        "test_set = test_set.reset_index(drop=True)\n",
        "\n",
        "test_set.to_csv('test_set.csv', index=False)\n",
        "print(f\"Test set created: {len(test_set)} rows\")\n",
        "print(f\"Saved to test_set.csv\")\n",
        "print(f\"\\nTest set preview:\")\n",
        "print(test_set.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Helper Functions and Model Setup\n",
        "\n",
        "### 2.1 Intersection over Union (IoU) Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    \n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    \n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    \n",
        "    unionArea = boxAArea + boxBArea - interArea\n",
        "    \n",
        "    if unionArea == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    iou = interArea / unionArea\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Visualization Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results(image, phrase, truth_box, pred_box, iou_score):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    ax.imshow(image)\n",
        "    \n",
        "    truth_rect = plt.Rectangle(\n",
        "        (truth_box[0], truth_box[1]),\n",
        "        truth_box[2] - truth_box[0],\n",
        "        truth_box[3] - truth_box[1],\n",
        "        linewidth=3, edgecolor='green', facecolor='none', label='Ground Truth'\n",
        "    )\n",
        "    ax.add_patch(truth_rect)\n",
        "    \n",
        "    pred_rect = plt.Rectangle(\n",
        "        (pred_box[0], pred_box[1]),\n",
        "        pred_box[2] - pred_box[0],\n",
        "        pred_box[3] - pred_box[1],\n",
        "        linewidth=3, edgecolor='red', facecolor='none', label='Model Prediction'\n",
        "    )\n",
        "    ax.add_patch(pred_rect)\n",
        "    \n",
        "    ax.set_title(f\"Phrase: '{phrase}' (IoU: {iou_score:.2f})\", fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Experiment 1 (Specialist Model: GLIP)\n",
        "\n",
        "### 3.1 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"VincentL/glip-base-patch16-224\"\n",
        "glip_processor = AutoProcessor.from_pretrained(model_id)\n",
        "glip_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "glip_model.to(device)\n",
        "glip_model.eval()\n",
        "print(f\"GLIP model loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Run Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set = pd.read_csv('test_set.csv')\n",
        "glip_results = []\n",
        "\n",
        "images_dir = \"data/flickr30k-images\"\n",
        "\n",
        "for idx, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Running GLIP\"):\n",
        "    image_filename = row['image_filename']\n",
        "    phrase = row['phrase']\n",
        "    truth_box = eval(row['bbox'])\n",
        "    \n",
        "    image_path = os.path.join(images_dir, image_filename)\n",
        "    if not os.path.exists(image_path):\n",
        "        continue\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    width, height = image.size\n",
        "    \n",
        "    inputs = glip_processor(text=phrase, images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = glip_model(**inputs)\n",
        "    \n",
        "    results = glip_processor.post_process_object_detection(outputs, threshold=0.0)\n",
        "    \n",
        "    if len(results) > 0 and len(results[0]['boxes']) > 0:\n",
        "        pred_box_normalized = results[0]['boxes'][0].cpu().numpy()\n",
        "        pred_box = [\n",
        "            int(pred_box_normalized[0] * width),\n",
        "            int(pred_box_normalized[1] * height),\n",
        "            int(pred_box_normalized[2] * width),\n",
        "            int(pred_box_normalized[3] * height)\n",
        "        ]\n",
        "    else:\n",
        "        pred_box = [0, 0, 0, 0]\n",
        "    \n",
        "    iou_score = calculate_iou(truth_box, pred_box)\n",
        "    \n",
        "    glip_results.append({\n",
        "        'image_filename': image_filename,\n",
        "        'phrase': phrase,\n",
        "        'truth_box': truth_box,\n",
        "        'pred_box': pred_box,\n",
        "        'iou': iou_score\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Report GLIP Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "glip_df = pd.DataFrame(glip_results)\n",
        "mean_iou = glip_df['iou'].mean()\n",
        "accuracy = (glip_df['iou'] > 0.5).sum() / len(glip_df) * 100\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"GLIP Results (Specialist Model)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "print(f\"Accuracy (IoU > 0.5): {accuracy:.2f}%\")\n",
        "print(f\"Total samples evaluated: {len(glip_df)}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Experiment 2 (Generalist Model: LLaVA)\n",
        "\n",
        "### 4.1 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llava_model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "llava_processor = LlavaProcessor.from_pretrained(llava_model_id)\n",
        "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    llava_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "llava_model.to(device)\n",
        "llava_model.eval()\n",
        "print(f\"LLaVA model loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Create Text-Parsing Helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_bbox_from_text(text):\n",
        "    pattern = r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return [int(match.group(1)), int(match.group(2)), int(match.group(3)), int(match.group(4))]\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Run Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set = pd.read_csv('test_set.csv')\n",
        "llava_results = []\n",
        "\n",
        "images_dir = \"data/flickr30k-images\"\n",
        "\n",
        "for idx, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Running LLaVA\"):\n",
        "    image_filename = row['image_filename']\n",
        "    phrase = row['phrase']\n",
        "    truth_box = eval(row['bbox'])\n",
        "    \n",
        "    image_path = os.path.join(images_dir, image_filename)\n",
        "    if not os.path.exists(image_path):\n",
        "        continue\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    prompt = f\"USER: <image>\\nWhat is the bounding box [xmin, ymin, xmax, ymax] for the phrase: '{phrase}'? Respond with *only* the bounding box.\\nASSISTANT:\"\n",
        "    \n",
        "    inputs = llava_processor(prompt, image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generate_ids = llava_model.generate(**inputs, max_new_tokens=20)\n",
        "    \n",
        "    generated_text = llava_processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    \n",
        "    pred_box = parse_bbox_from_text(generated_text)\n",
        "    \n",
        "    if pred_box is None:\n",
        "        iou_score = 0.0\n",
        "        pred_box = [0, 0, 0, 0]\n",
        "    else:\n",
        "        iou_score = calculate_iou(truth_box, pred_box)\n",
        "    \n",
        "    llava_results.append({\n",
        "        'image_filename': image_filename,\n",
        "        'phrase': phrase,\n",
        "        'truth_box': truth_box,\n",
        "        'pred_box': pred_box,\n",
        "        'iou': iou_score,\n",
        "        'generated_text': generated_text\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Report LLaVA Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llava_df = pd.DataFrame(llava_results)\n",
        "mean_iou = llava_df['iou'].mean()\n",
        "accuracy = (llava_df['iou'] > 0.5).sum() / len(llava_df) * 100\n",
        "format_failures = (llava_df['iou'] == 0.0).sum()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"LLaVA Results (Generalist Model)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "print(f\"Accuracy (IoU > 0.5): {accuracy:.2f}%\")\n",
        "print(f\"Format failures (no box output): {format_failures} ({format_failures/len(llava_df)*100:.2f}%)\")\n",
        "print(f\"Total samples evaluated: {len(llava_df)}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Final Comparison and Qualitative Analysis\n",
        "\n",
        "### 5.1 Quantitative Head-to-Head Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['GLIP (Specialist)', 'LLaVA (Generalist)'],\n",
        "    'Mean IoU': [glip_df['iou'].mean(), llava_df['iou'].mean()],\n",
        "    'Accuracy (IoU > 0.5)': [\n",
        "        (glip_df['iou'] > 0.5).sum() / len(glip_df) * 100,\n",
        "        (llava_df['iou'] > 0.5).sum() / len(llava_df) * 100\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"QUANTITATIVE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Qualitative Failure Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = pd.merge(\n",
        "    glip_df[['image_filename', 'phrase', 'truth_box', 'pred_box', 'iou']],\n",
        "    llava_df[['image_filename', 'phrase', 'pred_box', 'iou']],\n",
        "    on=['image_filename', 'phrase'],\n",
        "    suffixes=('_glip', '_llava')\n",
        ")\n",
        "\n",
        "images_dir = \"data/flickr30k-images\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Easy Success: Both models perform well (IoU > 0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "easy_success = merged_df[(merged_df['iou_glip'] > 0.7) & (merged_df['iou_llava'] > 0.7)]\n",
        "if len(easy_success) > 0:\n",
        "    sample = easy_success.iloc[0]\n",
        "    image_path = os.path.join(images_dir, sample['image_filename'])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Example: '{sample['phrase']}'\")\n",
        "    print(f\"GLIP IoU: {sample['iou_glip']:.3f}, LLaVA IoU: {sample['iou_llava']:.3f}\")\n",
        "    visualize_results(\n",
        "        image,\n",
        "        sample['phrase'],\n",
        "        eval(str(sample['truth_box'])) if isinstance(sample['truth_box'], str) else sample['truth_box'],\n",
        "        eval(str(sample['pred_box_llava'])) if isinstance(sample['pred_box_llava'], str) else sample['pred_box_llava'],\n",
        "        sample['iou_llava']\n",
        "    )\n",
        "else:\n",
        "    print(\"No easy success cases found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GLIP Win: GLIP succeeds but LLaVA fails (GLIP IoU > 0.7, LLaVA IoU < 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "glip_win = merged_df[(merged_df['iou_glip'] > 0.7) & (merged_df['iou_llava'] < 0.2)]\n",
        "if len(glip_win) > 0:\n",
        "    sample = glip_win.iloc[0]\n",
        "    image_path = os.path.join(images_dir, sample['image_filename'])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Example: '{sample['phrase']}'\")\n",
        "    print(f\"GLIP IoU: {sample['iou_glip']:.3f}, LLaVA IoU: {sample['iou_llava']:.3f}\")\n",
        "    print(\"Showing LLaVA's prediction (failure case):\")\n",
        "    visualize_results(\n",
        "        image,\n",
        "        sample['phrase'],\n",
        "        eval(str(sample['truth_box'])) if isinstance(sample['truth_box'], str) else sample['truth_box'],\n",
        "        eval(str(sample['pred_box_llava'])) if isinstance(sample['pred_box_llava'], str) else sample['pred_box_llava'],\n",
        "        sample['iou_llava']\n",
        "    )\n",
        "else:\n",
        "    print(\"No GLIP win cases found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Total Failure: Both models fail (IoU < 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_failure = merged_df[(merged_df['iou_glip'] < 0.2) & (merged_df['iou_llava'] < 0.2)]\n",
        "if len(total_failure) > 0:\n",
        "    sample = total_failure.iloc[0]\n",
        "    image_path = os.path.join(images_dir, sample['image_filename'])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Example: '{sample['phrase']}'\")\n",
        "    print(f\"GLIP IoU: {sample['iou_glip']:.3f}, LLaVA IoU: {sample['iou_llava']:.3f}\")\n",
        "    print(\"Showing LLaVA's prediction:\")\n",
        "    visualize_results(\n",
        "        image,\n",
        "        sample['phrase'],\n",
        "        eval(str(sample['truth_box'])) if isinstance(sample['truth_box'], str) else sample['truth_box'],\n",
        "        eval(str(sample['pred_box_llava'])) if isinstance(sample['pred_box_llava'], str) else sample['pred_box_llava'],\n",
        "        sample['iou_llava']\n",
        "    )\n",
        "else:\n",
        "    print(\"No total failure cases found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LLaVA Format Failure: LLaVA failed to output a bounding box (IoU = 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "format_failure = llava_df[llava_df['iou'] == 0.0]\n",
        "if len(format_failure) > 0:\n",
        "    sample = format_failure.iloc[0]\n",
        "    print(f\"Example phrase: '{sample['phrase']}'\")\n",
        "    print(f\"Generated text: {sample['generated_text']}\")\n",
        "    print(\"\\nThis demonstrates LLaVA's format compliance issues.\")\n",
        "else:\n",
        "    print(\"No format failure cases found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Final Conclusions\n",
        "\n",
        "Based on our quantitative and qualitative analysis, we can draw the following key findings:\n",
        "(IN PROGRESS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository\n",
        "\n",
        "This project is available at: https://github.com/krishnankonda/vlm-grounding-analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
